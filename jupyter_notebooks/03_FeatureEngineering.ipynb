{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554b520e",
   "metadata": {},
   "source": [
    "<div style='background-color:#9B59B6; padding: 15px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "<h1 style='color:#FFFFFF; text-align:center; font-family: Arial, sans-serif; margin: 0;'>üîß Feature Engineering & Preprocessing</h1>\n",
    "<h2 style='color:#E8DAEF; text-align:center; font-family: Arial, sans-serif; margin: 5px 0 0 0;'>Preparing Data for Machine Learning</h2>\n",
    "</div>\n",
    "\n",
    "<div style='background-color:#F8F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #9B59B6;'>\n",
    "<h3 style='color:#9B59B6; margin-top: 0;'>‚öôÔ∏è Engineering Objectives</h3>\n",
    "<ul style='color:#333; line-height: 1.6;'>\n",
    "<li><strong>Data Transformation:</strong> Convert categorical variables to numerical format</li>\n",
    "<li><strong>Feature Creation:</strong> Engineer new features from existing data</li>\n",
    "<li><strong>Data Scaling:</strong> Normalize numerical features for optimal model performance</li>\n",
    "<li><strong>Missing Value Handling:</strong> Implement robust imputation strategies</li>\n",
    "<li><strong>Feature Selection:</strong> Identify most predictive variables for stroke risk</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f95c63",
   "metadata": {},
   "source": [
    "# **Feature Engineering for Stroke Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18265cb8",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Engineer features for machine learning model training\n",
    "* Handle missing values and data preprocessing\n",
    "* Create derived features and encode categorical variables\n",
    "* Prepare train-test datasets with proper validation splits\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* Raw stroke dataset with EDA insights\n",
    "* Statistical analysis results from previous notebooks\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Cleaned and preprocessed dataset\n",
    "* Engineered features for model training\n",
    "* Train-test-validation splits\n",
    "* Feature encoding pipelines\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* Focus on creating features that improve model performance\n",
    "* Handle class imbalance considerations\n",
    "* Ensure data leakage prevention in preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c1186",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec178471",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99490dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Working directory changed to parent folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e8f38",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a752283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"inputs/datasets/Stroke-data.csv\")\n",
    "print(f\"Dataset loaded: {df.shape[0]} patients, {df.shape[1]} features\")\n",
    "print(f\"Stroke cases: {df['stroke'].sum()} ({df['stroke'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974b54b",
   "metadata": {},
   "source": [
    "## Data Quality Assessment and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"{col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate records: {duplicates}\")\n",
    "\n",
    "# Data type overview\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c411030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing BMI values\n",
    "print(f\"BMI missing values: {df['bmi'].isnull().sum()}\")\n",
    "\n",
    "# Analyze BMI missing pattern\n",
    "bmi_missing_analysis = df.groupby('stroke')['bmi'].agg(['count', lambda x: x.isnull().sum()])\n",
    "bmi_missing_analysis.columns = ['total', 'missing']\n",
    "bmi_missing_analysis['missing_rate'] = bmi_missing_analysis['missing'] / bmi_missing_analysis['total']\n",
    "print(\"\\nBMI missing pattern by stroke status:\")\n",
    "print(bmi_missing_analysis)\n",
    "\n",
    "# Strategy: Use median imputation grouped by key characteristics\n",
    "# Create age groups for better imputation\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], labels=['Young', 'Adult', 'MiddleAged', 'Senior'])\n",
    "\n",
    "# Impute BMI based on age group and gender\n",
    "for age_group in df['age_group'].unique():\n",
    "    for gender in df['gender'].unique():\n",
    "        mask = (df['age_group'] == age_group) & (df['gender'] == gender)\n",
    "        median_bmi = df.loc[mask, 'bmi'].median()\n",
    "        df.loc[mask & df['bmi'].isnull(), 'bmi'] = median_bmi\n",
    "\n",
    "print(f\"\\nBMI missing values after imputation: {df['bmi'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f66c5a",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb89899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features based on EDA insights\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# 1. Age categories (based on stroke risk patterns)\n",
    "df['age_category'] = pd.cut(df['age'], \n",
    "                           bins=[0, 40, 55, 70, 100], \n",
    "                           labels=['Low_Risk', 'Moderate_Risk', 'High_Risk', 'Very_High_Risk'])\n",
    "\n",
    "# 2. BMI categories (WHO standards)\n",
    "df['bmi_category'] = pd.cut(df['bmi'], \n",
    "                           bins=[0, 18.5, 25, 30, 100], \n",
    "                           labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "\n",
    "# 3. Glucose level categories (diabetes risk)\n",
    "df['glucose_category'] = pd.cut(df['avg_glucose_level'], \n",
    "                               bins=[0, 100, 125, 1000], \n",
    "                               labels=['Normal', 'Prediabetic', 'Diabetic'])\n",
    "\n",
    "# 4. Health risk score (composite feature)\n",
    "df['health_risk_score'] = (\n",
    "    df['hypertension'] * 2 +  # Hypertension contributes 2 points\n",
    "    df['heart_disease'] * 3 +  # Heart disease contributes 3 points\n",
    "    (df['avg_glucose_level'] > 125).astype(int) * 2 +  # Diabetes contributes 2 points\n",
    "    (df['bmi'] > 30).astype(int) * 1  # Obesity contributes 1 point\n",
    ")\n",
    "\n",
    "# 5. Age-health interaction\n",
    "df['age_health_interaction'] = df['age'] * (df['hypertension'] + df['heart_disease'])\n",
    "\n",
    "# 6. Lifestyle risk score\n",
    "smoking_risk = df['smoking_status'].map({\n",
    "    'never smoked': 0,\n",
    "    'Unknown': 1,\n",
    "    'formerly smoked': 2,\n",
    "    'smokes': 3\n",
    "})\n",
    "df['lifestyle_risk_score'] = smoking_risk + (df['bmi'] > 30).astype(int)\n",
    "\n",
    "print(\"Derived features created:\")\n",
    "derived_features = ['age_category', 'bmi_category', 'glucose_category', \n",
    "                   'health_risk_score', 'age_health_interaction', 'lifestyle_risk_score']\n",
    "for feature in derived_features:\n",
    "    print(f\"- {feature}\")\n",
    "    if df[feature].dtype == 'object' or hasattr(df[feature], 'cat'):\n",
    "        print(f\"  Categories: {df[feature].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(f\"  Range: {df[feature].min():.1f} - {df[feature].max():.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a116b85",
   "metadata": {},
   "source": [
    "## Feature Selection and Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "# Original features\n",
    "numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "categorical_features = ['gender', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                       'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "# Derived features\n",
    "derived_numerical = ['health_risk_score', 'age_health_interaction', 'lifestyle_risk_score']\n",
    "derived_categorical = ['age_category', 'bmi_category', 'glucose_category']\n",
    "\n",
    "# Combine all features\n",
    "all_numerical = numerical_features + derived_numerical\n",
    "all_categorical = categorical_features + derived_categorical\n",
    "\n",
    "print(f\"Total features for modeling:\")\n",
    "print(f\"Numerical features ({len(all_numerical)}): {all_numerical}\")\n",
    "print(f\"Categorical features ({len(all_categorical)}): {all_categorical}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "# Numerical features: imputation + scaling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features: imputation + one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, all_numerical),\n",
    "        ('cat', categorical_transformer, all_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202e8ba",
   "metadata": {},
   "source": [
    "## Train-Test Split and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_columns = all_numerical + all_categorical\n",
    "X = df[feature_columns].copy()\n",
    "y = df['stroke'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Stratified train-test split to maintain class balance\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% stroke rate)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({y_val.mean()*100:.1f}% stroke rate)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% stroke rate)\")\n",
    "\n",
    "# Verify class balance preservation\n",
    "print(f\"\\nClass balance verification:\")\n",
    "print(f\"Original: {y.mean():.3f}\")\n",
    "print(f\"Train: {y_train.mean():.3f}\")\n",
    "print(f\"Validation: {y_val.mean():.3f}\")\n",
    "print(f\"Test: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec394804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessing pipeline on training data only\n",
    "print(\"Fitting preprocessing pipeline...\")\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature shapes:\")\n",
    "print(f\"Training: {X_train_processed.shape}\")\n",
    "print(f\"Validation: {X_val_processed.shape}\")\n",
    "print(f\"Test: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "# Numerical feature names remain the same\n",
    "num_feature_names = all_numerical\n",
    "\n",
    "# Get categorical feature names after one-hot encoding\n",
    "cat_feature_names = list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(all_categorical))\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "print(f\"\\nTotal features after preprocessing: {len(all_feature_names)}\")\n",
    "print(f\"Numerical features: {len(num_feature_names)}\")\n",
    "print(f\"Categorical features (one-hot encoded): {len(cat_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c7efc",
   "metadata": {},
   "source": [
    "## Feature Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc07474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance of derived features\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "\n",
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "\n",
    "# 1. Health risk score analysis\n",
    "health_risk_stroke = df.groupby('health_risk_score')['stroke'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\nHealth Risk Score vs Stroke Rate:\")\n",
    "for score, data in health_risk_stroke.iterrows():\n",
    "    print(f\"Score {score}: {data['mean']:.3f} ({data['mean']*100:.1f}%) - {data['sum']}/{data['count']} patients\")\n",
    "\n",
    "# 2. Age category analysis\n",
    "age_cat_stroke = df.groupby('age_category')['stroke'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\nAge Category vs Stroke Rate:\")\n",
    "for category, data in age_cat_stroke.iterrows():\n",
    "    print(f\"{category}: {data['mean']:.3f} ({data['mean']*100:.1f}%) - {data['sum']}/{data['count']} patients\")\n",
    "\n",
    "# 3. Glucose category analysis\n",
    "glucose_cat_stroke = df.groupby('glucose_category')['stroke'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\nGlucose Category vs Stroke Rate:\")\n",
    "for category, data in glucose_cat_stroke.iterrows():\n",
    "    print(f\"{category}: {data['mean']:.3f} ({data['mean']*100:.1f}%) - {data['sum']}/{data['count']} patients\")\n",
    "\n",
    "# Statistical validation of derived features\n",
    "health_risk_corr, health_risk_p = pearsonr(df['health_risk_score'], df['stroke'])\n",
    "lifestyle_risk_corr, lifestyle_risk_p = pearsonr(df['lifestyle_risk_score'], df['stroke'])\n",
    "age_health_corr, age_health_p = pearsonr(df['age_health_interaction'], df['stroke'])\n",
    "\n",
    "print(\"\\n=== DERIVED FEATURE CORRELATIONS ===\")\n",
    "print(f\"Health Risk Score: r={health_risk_corr:.3f}, p={health_risk_p:.2e}\")\n",
    "print(f\"Lifestyle Risk Score: r={lifestyle_risk_corr:.3f}, p={lifestyle_risk_p:.2e}\")\n",
    "print(f\"Age-Health Interaction: r={age_health_corr:.3f}, p={age_health_p:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5f93f",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc878de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data quality checks\n",
    "print(\"=== FINAL DATA QUALITY VALIDATION ===\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"Missing values in processed training data: {np.isnan(X_train_processed).sum()}\")\n",
    "print(f\"Missing values in processed validation data: {np.isnan(X_val_processed).sum()}\")\n",
    "print(f\"Missing values in processed test data: {np.isnan(X_test_processed).sum()}\")\n",
    "\n",
    "# Check for infinite values\n",
    "print(f\"Infinite values in training data: {np.isinf(X_train_processed).sum()}\")\n",
    "\n",
    "# Check feature scaling (for numerical features)\n",
    "print(f\"\\nNumerical feature scaling check (training data):\")\n",
    "for i, feature in enumerate(num_feature_names):\n",
    "    feature_data = X_train_processed[:, i]\n",
    "    print(f\"{feature}: mean={feature_data.mean():.3f}, std={feature_data.std():.3f}\")\n",
    "\n",
    "# Verify no data leakage (statistics should be similar but not identical)\n",
    "print(f\"\\nData leakage check - feature means:\")\n",
    "train_means = X_train_processed.mean(axis=0)[:len(num_feature_names)]\n",
    "val_means = X_val_processed.mean(axis=0)[:len(num_feature_names)]\n",
    "test_means = X_test_processed.mean(axis=0)[:len(num_feature_names)]\n",
    "\n",
    "for i, feature in enumerate(num_feature_names):\n",
    "    print(f\"{feature}: train={train_means[i]:.3f}, val={val_means[i]:.3f}, test={test_means[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821b336",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdc9ff",
   "metadata": {},
   "source": [
    "# Save Processed Data and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessing pipeline\n",
    "joblib.dump(preprocessor, 'outputs/ml_pipeline/feature_engineering_pipeline.pkl')\n",
    "print(\"Preprocessing pipeline saved successfully!\")\n",
    "\n",
    "# Save processed datasets\n",
    "np.save('outputs/datasets/X_train_processed.npy', X_train_processed)\n",
    "np.save('outputs/datasets/X_val_processed.npy', X_val_processed)\n",
    "np.save('outputs/datasets/X_test_processed.npy', X_test_processed)\n",
    "np.save('outputs/datasets/y_train.npy', y_train.values)\n",
    "np.save('outputs/datasets/y_val.npy', y_val.values)\n",
    "np.save('outputs/datasets/y_test.npy', y_test.values)\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'all_feature_names': all_feature_names,\n",
    "    'numerical_features': all_numerical,\n",
    "    'categorical_features': all_categorical,\n",
    "    'original_features': feature_columns,\n",
    "    'n_features_after_preprocessing': len(all_feature_names),\n",
    "    'train_shape': X_train_processed.shape,\n",
    "    'val_shape': X_val_processed.shape,\n",
    "    'test_shape': X_test_processed.shape\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('outputs/datasets/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "# Save dataset with all engineered features for further analysis\n",
    "df_with_features = df.copy()\n",
    "df_with_features.to_csv('outputs/datasets/stroke_engineered_features.csv', index=False)\n",
    "\n",
    "print(\"\\n=== FEATURE ENGINEERING COMPLETE ===\")\n",
    "print(f\"Original features: {len(feature_columns)}\")\n",
    "print(f\"Features after preprocessing: {len(all_feature_names)}\")\n",
    "print(f\"Training samples: {X_train_processed.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val_processed.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_processed.shape[0]}\")\n",
    "print(f\"Class balance maintained: {abs(y_train.mean() - y.mean()) < 0.01}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- Preprocessing pipeline: outputs/ml_pipeline/feature_engineering_pipeline.pkl\")\n",
    "print(\"- Processed datasets: outputs/datasets/X_*.npy, y_*.npy\")\n",
    "print(\"- Feature information: outputs/datasets/feature_info.json\")\n",
    "print(\"- Enhanced dataset: outputs/datasets/stroke_engineered_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
