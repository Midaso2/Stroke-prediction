{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554b520e",
   "metadata": {},
   "source": [
    "<div style='background-color:#9B59B6; padding: 15px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "<h1 style='color:#FFFFFF; text-align:center; font-family: Arial, sans-serif; margin: 0;'>üîß Feature Engineering & Preprocessing</h1>\n",
    "<h2 style='color:#E8DAEF; text-align:center; font-family: Arial, sans-serif; margin: 5px 0 0 0;'>Preparing Data for Machine Learning</h2>\n",
    "</div>\n",
    "\n",
    "<div style='background-color:#F8F5FF; padding: 15px; border-radius: 8px; border-left: 4px solid #9B59B6;'>\n",
    "<h3 style='color:#9B59B6; margin-top: 0;'>‚öôÔ∏è Engineering Objectives</h3>\n",
    "<ul style='color:#333; line-height: 1.6;'>\n",
    "<li><strong>Data Transformation:</strong> Convert categorical variables to numerical format</li>\n",
    "<li><strong>Feature Creation:</strong> Engineer new features from existing data</li>\n",
    "<li><strong>Data Scaling:</strong> Normalize numerical features for optimal model performance</li>\n",
    "<li><strong>Missing Value Handling:</strong> Implement robust imputation strategies</li>\n",
    "<li><strong>Feature Selection:</strong> Identify most predictive variables for stroke risk</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f95c63",
   "metadata": {},
   "source": [
    "# **Feature Engineering for Stroke Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18265cb8",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Engineer features for machine learning model training\n",
    "* Handle missing values and data preprocessing\n",
    "* Create derived features and encode categorical variables\n",
    "* Prepare train-test datasets with proper validation splits\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* Raw stroke dataset with EDA insights\n",
    "* Statistical analysis results from previous notebooks\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Cleaned and preprocessed dataset\n",
    "* Engineered features for model training\n",
    "* Train-test-validation splits\n",
    "* Feature encoding pipelines\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* Focus on creating features that improve model performance\n",
    "* Handle class imbalance considerations\n",
    "* Ensure data leakage prevention in preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c1186",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec178471",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99490dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory changed to parent folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"Working directory changed to parent folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e8f38",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a752283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 5110 patients, 12 features\n",
      "Stroke cases: 249 (4.9%)\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5110 non-null   int64  \n",
      " 1   gender             5110 non-null   object \n",
      " 2   age                5110 non-null   float64\n",
      " 3   hypertension       5110 non-null   int64  \n",
      " 4   heart_disease      5110 non-null   int64  \n",
      " 5   ever_married       5110 non-null   object \n",
      " 6   work_type          5110 non-null   object \n",
      " 7   Residence_type     5110 non-null   object \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                4909 non-null   float64\n",
      " 10  smoking_status     5110 non-null   object \n",
      " 11  stroke             5110 non-null   int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 479.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset - use the correct path\n",
    "df = pd.read_csv(\"jupyter_notebooks/inputs/datasets/Stroke-data.csv\")\n",
    "print(f\"Dataset loaded: {df.shape[0]} patients, {df.shape[1]} features\")\n",
    "print(f\"Stroke cases: {df['stroke'].sum()} ({df['stroke'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974b54b",
   "metadata": {},
   "source": [
    "## Data Quality Assessment and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb55b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "bmi: 201 (3.9%)\n",
      "\n",
      "Duplicate records: 0\n",
      "\n",
      "Data types:\n",
      "id                     int64\n",
      "gender                object\n",
      "age                  float64\n",
      "hypertension           int64\n",
      "heart_disease          int64\n",
      "ever_married          object\n",
      "work_type             object\n",
      "Residence_type        object\n",
      "avg_glucose_level    float64\n",
      "bmi                  float64\n",
      "smoking_status        object\n",
      "stroke                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"{col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate records: {duplicates}\")\n",
    "\n",
    "# Data type overview\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c411030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI missing values: 201\n",
      "\n",
      "BMI missing pattern by stroke status:\n",
      "        total  missing  missing_rate\n",
      "stroke                              \n",
      "0        4700      161      0.034255\n",
      "1         209       40      0.191388\n",
      "\n",
      "BMI missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing BMI values\n",
    "print(f\"BMI missing values: {df['bmi'].isnull().sum()}\")\n",
    "\n",
    "# Analyze BMI missing pattern\n",
    "bmi_missing_analysis = df.groupby('stroke')['bmi'].agg(['count', lambda x: x.isnull().sum()])\n",
    "bmi_missing_analysis.columns = ['total', 'missing']\n",
    "bmi_missing_analysis['missing_rate'] = bmi_missing_analysis['missing'] / bmi_missing_analysis['total']\n",
    "print(\"\\nBMI missing pattern by stroke status:\")\n",
    "print(bmi_missing_analysis)\n",
    "\n",
    "# Strategy: Use median imputation grouped by key characteristics\n",
    "# Create age groups for better imputation\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], labels=['Young', 'Adult', 'MiddleAged', 'Senior'])\n",
    "\n",
    "# Impute BMI based on age group and gender\n",
    "for age_group in df['age_group'].unique():\n",
    "    for gender in df['gender'].unique():\n",
    "        mask = (df['age_group'] == age_group) & (df['gender'] == gender)\n",
    "        median_bmi = df.loc[mask, 'bmi'].median()\n",
    "        df.loc[mask & df['bmi'].isnull(), 'bmi'] = median_bmi\n",
    "\n",
    "print(f\"\\nBMI missing values after imputation: {df['bmi'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f66c5a",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb89899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating derived features...\n",
      "Derived features created:\n",
      "- age_category\n",
      "  Categories: {'Low_Risk': 2244, 'Moderate_Risk': 1170, 'High_Risk': 986, 'Very_High_Risk': 710}\n",
      "\n",
      "- bmi_category\n",
      "  Categories: {'Obese': 1941, 'Overweight': 1528, 'Normal': 1292, 'Underweight': 349}\n",
      "\n",
      "- glucose_category\n",
      "  Categories: {'Normal': 3131, 'Diabetic': 1000, 'Prediabetic': 979}\n",
      "\n",
      "- health_risk_score\n",
      "  Range: 0.0 - 8.0\n",
      "\n",
      "- age_health_interaction\n",
      "  Range: 0.0 - 164.0\n",
      "\n",
      "- lifestyle_risk_score\n",
      "  Range: 0.0 - 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create derived features based on EDA insights\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# 1. Age categories (based on stroke risk patterns)\n",
    "df['age_category'] = pd.cut(df['age'], \n",
    "                           bins=[0, 40, 55, 70, 100], \n",
    "                           labels=['Low_Risk', 'Moderate_Risk', 'High_Risk', 'Very_High_Risk'])\n",
    "\n",
    "# 2. BMI categories (WHO standards)\n",
    "df['bmi_category'] = pd.cut(df['bmi'], \n",
    "                           bins=[0, 18.5, 25, 30, 100], \n",
    "                           labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "\n",
    "# 3. Glucose level categories (diabetes risk)\n",
    "df['glucose_category'] = pd.cut(df['avg_glucose_level'], \n",
    "                               bins=[0, 100, 125, 1000], \n",
    "                               labels=['Normal', 'Prediabetic', 'Diabetic'])\n",
    "\n",
    "# 4. Health risk score (composite feature)\n",
    "df['health_risk_score'] = (\n",
    "    df['hypertension'] * 2 +  # Hypertension contributes 2 points\n",
    "    df['heart_disease'] * 3 +  # Heart disease contributes 3 points\n",
    "    (df['avg_glucose_level'] > 125).astype(int) * 2 +  # Diabetes contributes 2 points\n",
    "    (df['bmi'] > 30).astype(int) * 1  # Obesity contributes 1 point\n",
    ")\n",
    "\n",
    "# 5. Age-health interaction\n",
    "df['age_health_interaction'] = df['age'] * (df['hypertension'] + df['heart_disease'])\n",
    "\n",
    "# 6. Lifestyle risk score\n",
    "smoking_risk = df['smoking_status'].map({\n",
    "    'never smoked': 0,\n",
    "    'Unknown': 1,\n",
    "    'formerly smoked': 2,\n",
    "    'smokes': 3\n",
    "})\n",
    "df['lifestyle_risk_score'] = smoking_risk + (df['bmi'] > 30).astype(int)\n",
    "\n",
    "print(\"Derived features created:\")\n",
    "derived_features = ['age_category', 'bmi_category', 'glucose_category', \n",
    "                   'health_risk_score', 'age_health_interaction', 'lifestyle_risk_score']\n",
    "for feature in derived_features:\n",
    "    print(f\"- {feature}\")\n",
    "    if df[feature].dtype == 'object' or hasattr(df[feature], 'cat'):\n",
    "        print(f\"  Categories: {df[feature].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(f\"  Range: {df[feature].min():.1f} - {df[feature].max():.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a116b85",
   "metadata": {},
   "source": [
    "## Feature Selection and Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30f98bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features for modeling:\n",
      "Numerical features (6): ['age', 'avg_glucose_level', 'bmi', 'health_risk_score', 'age_health_interaction', 'lifestyle_risk_score']\n",
      "Categorical features (10): ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'age_category', 'bmi_category', 'glucose_category']\n",
      "\n",
      "Preprocessing pipeline created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define feature sets\n",
    "# Original features\n",
    "numerical_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "categorical_features = ['gender', 'hypertension', 'heart_disease', 'ever_married', \n",
    "                       'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "# Derived features\n",
    "derived_numerical = ['health_risk_score', 'age_health_interaction', 'lifestyle_risk_score']\n",
    "derived_categorical = ['age_category', 'bmi_category', 'glucose_category']\n",
    "\n",
    "# Combine all features\n",
    "all_numerical = numerical_features + derived_numerical\n",
    "all_categorical = categorical_features + derived_categorical\n",
    "\n",
    "print(f\"Total features for modeling:\")\n",
    "print(f\"Numerical features ({len(all_numerical)}): {all_numerical}\")\n",
    "print(f\"Categorical features ({len(all_categorical)}): {all_categorical}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "# Numerical features: imputation + scaling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical features: imputation + one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, all_numerical),\n",
    "        ('cat', categorical_transformer, all_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202e8ba",
   "metadata": {},
   "source": [
    "## Train-Test Split and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0228fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5110, 16)\n",
      "Target vector shape: (5110,)\n",
      "Class distribution: {0: 4861, 1: 249}\n",
      "\n",
      "Dataset splits:\n",
      "Training set: 3066 samples (4.9% stroke rate)\n",
      "Validation set: 1022 samples (4.9% stroke rate)\n",
      "Test set: 1022 samples (4.9% stroke rate)\n",
      "\n",
      "Class balance verification:\n",
      "Original: 0.049\n",
      "Train: 0.049\n",
      "Validation: 0.049\n",
      "Test: 0.049\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "feature_columns = all_numerical + all_categorical\n",
    "X = df[feature_columns].copy()\n",
    "y = df['stroke'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Stratified train-test split to maintain class balance\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({y_train.mean()*100:.1f}% stroke rate)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({y_val.mean()*100:.1f}% stroke rate)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({y_test.mean()*100:.1f}% stroke rate)\")\n",
    "\n",
    "# Verify class balance preservation\n",
    "print(f\"\\nClass balance verification:\")\n",
    "print(f\"Original: {y.mean():.3f}\")\n",
    "print(f\"Train: {y_train.mean():.3f}\")\n",
    "print(f\"Validation: {y_val.mean():.3f}\")\n",
    "print(f\"Test: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec394804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting preprocessing pipeline...\n",
      "Processed feature shapes:\n",
      "Training: (3066, 27)\n",
      "Validation: (1022, 27)\n",
      "Test: (1022, 27)\n",
      "\n",
      "Total features after preprocessing: 27\n",
      "Numerical features: 6\n",
      "Categorical features (one-hot encoded): 21\n"
     ]
    }
   ],
   "source": [
    "# Fit preprocessing pipeline on training data only\n",
    "print(\"Fitting preprocessing pipeline...\")\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed feature shapes:\")\n",
    "print(f\"Training: {X_train_processed.shape}\")\n",
    "print(f\"Validation: {X_val_processed.shape}\")\n",
    "print(f\"Test: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "# Numerical feature names remain the same\n",
    "num_feature_names = all_numerical\n",
    "\n",
    "# Get categorical feature names after one-hot encoding\n",
    "cat_feature_names = list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(all_categorical))\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "print(f\"\\nTotal features after preprocessing: {len(all_feature_names)}\")\n",
    "print(f\"Numerical features: {len(num_feature_names)}\")\n",
    "print(f\"Categorical features (one-hot encoded): {len(cat_feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c7efc",
   "metadata": {},
   "source": [
    "## Feature Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc07474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ANALYSIS ===\n",
      "\n",
      "Health Risk Score vs Stroke Rate:\n",
      "Score 0: 0.029 (2.9%) - 72.0/2449.0 patients\n",
      "Score 1: 0.025 (2.5%) - 30.0/1221.0 patients\n",
      "Score 2: 0.078 (7.8%) - 40.0/513.0 patients\n",
      "Score 3: 0.076 (7.6%) - 44.0/579.0 patients\n",
      "Score 4: 0.167 (16.7%) - 17.0/102.0 patients\n",
      "Score 5: 0.166 (16.6%) - 25.0/151.0 patients\n",
      "Score 6: 0.213 (21.3%) - 13.0/61.0 patients\n",
      "Score 7: 0.150 (15.0%) - 3.0/20.0 patients\n",
      "Score 8: 0.357 (35.7%) - 5.0/14.0 patients\n",
      "\n",
      "Age Category vs Stroke Rate:\n",
      "Low_Risk: 0.004 (0.4%) - 8.0/2244.0 patients\n",
      "Moderate_Risk: 0.026 (2.6%) - 31.0/1170.0 patients\n",
      "High_Risk: 0.083 (8.3%) - 82.0/986.0 patients\n",
      "Very_High_Risk: 0.180 (18.0%) - 128.0/710.0 patients\n",
      "\n",
      "Glucose Category vs Stroke Rate:\n",
      "Normal: 0.036 (3.6%) - 112.0/3131.0 patients\n",
      "Prediabetic: 0.038 (3.8%) - 37.0/979.0 patients\n",
      "Diabetic: 0.100 (10.0%) - 100.0/1000.0 patients\n",
      "\n",
      "=== DERIVED FEATURE CORRELATIONS ===\n",
      "Health Risk Score: r=0.178, p=1.33e-37\n",
      "Lifestyle Risk Score: r=0.032, p=2.07e-02\n",
      "Age-Health Interaction: r=0.194, p=2.65e-44\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature importance of derived features\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "\n",
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "\n",
    "# 1. Health risk score analysis\n",
    "health_risk_stroke = df.groupby('health_risk_score')['stroke'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\nHealth Risk Score vs Stroke Rate:\")\n",
    "for score, data in health_risk_stroke.iterrows():\n",
    "    print(f\"Score {score}: {data['mean']:.3f} ({data['mean']*100:.1f}%) - {data['sum']}/{data['count']} patients\")\n",
    "\n",
    "# 2. Age category analysis\n",
    "age_cat_stroke = df.groupby('age_category')['stroke'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\nAge Category vs Stroke Rate:\")\n",
    "for category, data in age_cat_stroke.iterrows():\n",
    "    print(f\"{category}: {data['mean']:.3f} ({data['mean']*100:.1f}%) - {data['sum']}/{data['count']} patients\")\n",
    "\n",
    "# 3. Glucose category analysis\n",
    "glucose_cat_stroke = df.groupby('glucose_category')['stroke'].agg(['count', 'sum', 'mean'])\n",
    "print(\"\\nGlucose Category vs Stroke Rate:\")\n",
    "for category, data in glucose_cat_stroke.iterrows():\n",
    "    print(f\"{category}: {data['mean']:.3f} ({data['mean']*100:.1f}%) - {data['sum']}/{data['count']} patients\")\n",
    "\n",
    "# Statistical validation of derived features\n",
    "health_risk_corr, health_risk_p = pearsonr(df['health_risk_score'], df['stroke'])\n",
    "lifestyle_risk_corr, lifestyle_risk_p = pearsonr(df['lifestyle_risk_score'], df['stroke'])\n",
    "age_health_corr, age_health_p = pearsonr(df['age_health_interaction'], df['stroke'])\n",
    "\n",
    "print(\"\\n=== DERIVED FEATURE CORRELATIONS ===\")\n",
    "print(f\"Health Risk Score: r={health_risk_corr:.3f}, p={health_risk_p:.2e}\")\n",
    "print(f\"Lifestyle Risk Score: r={lifestyle_risk_corr:.3f}, p={lifestyle_risk_p:.2e}\")\n",
    "print(f\"Age-Health Interaction: r={age_health_corr:.3f}, p={age_health_p:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5f93f",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc878de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL DATA QUALITY VALIDATION ===\n",
      "Missing values in processed training data: 0\n",
      "Missing values in processed validation data: 0\n",
      "Missing values in processed test data: 0\n",
      "Infinite values in training data: 0\n",
      "\n",
      "Numerical feature scaling check (training data):\n",
      "age: mean=0.000, std=1.000\n",
      "avg_glucose_level: mean=0.000, std=1.000\n",
      "bmi: mean=0.000, std=1.000\n",
      "health_risk_score: mean=-0.000, std=1.000\n",
      "age_health_interaction: mean=0.000, std=1.000\n",
      "lifestyle_risk_score: mean=0.000, std=1.000\n",
      "\n",
      "Data leakage check - feature means:\n",
      "age: train=0.000, val=-0.000, test=-0.028\n",
      "avg_glucose_level: train=0.000, val=-0.038, test=-0.028\n",
      "bmi: train=0.000, val=-0.044, test=-0.029\n",
      "health_risk_score: train=0.000, val=-0.033, test=-0.022\n",
      "age_health_interaction: train=-0.000, val=0.006, test=0.005\n",
      "lifestyle_risk_score: train=0.000, val=-0.012, test=-0.021\n"
     ]
    }
   ],
   "source": [
    "# Final data quality checks\n",
    "print(\"=== FINAL DATA QUALITY VALIDATION ===\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"Missing values in processed training data: {np.isnan(X_train_processed).sum()}\")\n",
    "print(f\"Missing values in processed validation data: {np.isnan(X_val_processed).sum()}\")\n",
    "print(f\"Missing values in processed test data: {np.isnan(X_test_processed).sum()}\")\n",
    "\n",
    "# Check for infinite values\n",
    "print(f\"Infinite values in training data: {np.isinf(X_train_processed).sum()}\")\n",
    "\n",
    "# Check feature scaling (for numerical features)\n",
    "print(f\"\\nNumerical feature scaling check (training data):\")\n",
    "for i, feature in enumerate(num_feature_names):\n",
    "    feature_data = X_train_processed[:, i]\n",
    "    print(f\"{feature}: mean={feature_data.mean():.3f}, std={feature_data.std():.3f}\")\n",
    "\n",
    "# Verify no data leakage (statistics should be similar but not identical)\n",
    "print(f\"\\nData leakage check - feature means:\")\n",
    "train_means = X_train_processed.mean(axis=0)[:len(num_feature_names)]\n",
    "val_means = X_val_processed.mean(axis=0)[:len(num_feature_names)]\n",
    "test_means = X_test_processed.mean(axis=0)[:len(num_feature_names)]\n",
    "\n",
    "for i, feature in enumerate(num_feature_names):\n",
    "    print(f\"{feature}: train={train_means[i]:.3f}, val={val_means[i]:.3f}, test={test_means[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821b336",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdc9ff",
   "metadata": {},
   "source": [
    "# Save Processed Data and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d289c50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All datasets and pipelines saved successfully!\n",
      "üìä Processed data shape: (3066, 27)\n",
      "üéØ Features created: 27\n",
      "üìÅ Files saved to: jupyter_notebooks/outputs/datasets/\n",
      "   - processed_stroke_data.npz\n",
      "   - preprocessing_pipeline.pkl\n",
      "   - feature_engineering_summary.json\n",
      "\n",
      "üéâ Feature Engineering Complete!\n",
      "Ready for machine learning model training!\n",
      "\n",
      "Dataset summary:\n",
      "  - Original features: 12\n",
      "  - Engineered features: 27\n",
      "  - Training samples: 3066\n",
      "  - Validation samples: 1022\n",
      "  - Test samples: 1022\n",
      "  - Class balance maintained: 0.049\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets and preprocessing pipeline\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"jupyter_notebooks/outputs/datasets\", exist_ok=True)\n",
    "\n",
    "# Create simple feature names based on the shape\n",
    "total_features = X_train_processed.shape[1]\n",
    "feature_names = [f\"feature_{i}\" for i in range(total_features)]\n",
    "\n",
    "# Create more descriptive names for the first few known features\n",
    "descriptive_names = numerical_features.copy()\n",
    "# Add one-hot encoded categorical feature names\n",
    "for cat_feature in categorical_features:\n",
    "    unique_values = df[cat_feature].unique()\n",
    "    for value in unique_values:\n",
    "        descriptive_names.append(f\"{cat_feature}_{value}\")\n",
    "\n",
    "# Use descriptive names up to the total number we have\n",
    "feature_names = descriptive_names[:total_features] + feature_names[len(descriptive_names):]\n",
    "\n",
    "# Save processed datasets\n",
    "np.savez('jupyter_notebooks/outputs/datasets/processed_stroke_data.npz',\n",
    "         X_train=X_train_processed, X_val=X_val_processed, X_test=X_test_processed,\n",
    "         y_train=y_train, y_val=y_val, y_test=y_test)\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "with open('jupyter_notebooks/outputs/datasets/preprocessing_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'preprocessor': preprocessor,\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features\n",
    "    }, f)\n",
    "\n",
    "# Save feature engineering summary\n",
    "summary = {\n",
    "    'total_features': total_features,\n",
    "    'numerical_features': len(numerical_features),\n",
    "    'categorical_features_original': len(categorical_features),\n",
    "    'training_samples': X_train_processed.shape[0],\n",
    "    'validation_samples': X_val_processed.shape[0],\n",
    "    'test_samples': X_test_processed.shape[0],\n",
    "    'stroke_rate_train': float(y_train.mean()),\n",
    "    'stroke_rate_val': float(y_val.mean()),\n",
    "    'stroke_rate_test': float(y_test.mean()),\n",
    "    'missing_values_imputed': 201,\n",
    "    'new_features_created': 6  # health_risk_score, age_health_interaction, etc.\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('jupyter_notebooks/outputs/datasets/feature_engineering_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ All datasets and pipelines saved successfully!\")\n",
    "print(f\"üìä Processed data shape: {X_train_processed.shape}\")\n",
    "print(f\"üéØ Features created: {total_features}\")\n",
    "print(f\"üìÅ Files saved to: jupyter_notebooks/outputs/datasets/\")\n",
    "print(f\"   - processed_stroke_data.npz\")\n",
    "print(f\"   - preprocessing_pipeline.pkl\")\n",
    "print(f\"   - feature_engineering_summary.json\")\n",
    "\n",
    "print(\"\\nüéâ Feature Engineering Complete!\")\n",
    "print(\"Ready for machine learning model training!\")\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  - Original features: 12\")\n",
    "print(f\"  - Engineered features: {total_features}\")\n",
    "print(f\"  - Training samples: {X_train_processed.shape[0]}\")\n",
    "print(f\"  - Validation samples: {X_val_processed.shape[0]}\")\n",
    "print(f\"  - Test samples: {X_test_processed.shape[0]}\")\n",
    "print(f\"  - Class balance maintained: {y_train.mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
